{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11964484,"sourceType":"datasetVersion","datasetId":7523376}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"e31d6502","cell_type":"code","source":"import pandas as pd\nfrom bs4 import BeautifulSoup\nimport os\n\n# Chemin du dossier de base sur Kaggle\nbase_dir = \"/kaggle/input/test-audio\"\n\n\n# Chargement du fichier CSV\ndf = pd.read_csv(f\"{base_dir}/Audio/metadata_balises_h1_p.csv\")\n\nfor index, row in df.iterrows():\n    # Construction du chemin absolu vers le fichier audio\n    audio_path = os.path.join(base_dir, row[\"audio\"])\n\n    # Vérifie l'existence du fichier audio\n    if not os.path.exists(audio_path):\n        print(f\"Fichier manquant : {audio_path}\")\n\n    # Vérifie la validité du HTML\n    try:\n        BeautifulSoup(row[\"html\"], 'html.parser')\n    except Exception as e:\n        print(f\"HTML invalide à la ligne {index} : {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T11:31:31.079126Z","iopub.execute_input":"2025-06-04T11:31:31.079677Z","iopub.status.idle":"2025-06-04T11:31:31.716544Z","shell.execute_reply.started":"2025-06-04T11:31:31.079654Z","shell.execute_reply":"2025-06-04T11:31:31.715700Z"}},"outputs":[],"execution_count":1},{"id":"4ab5a347","cell_type":"code","source":"import pandas as pd\nimport librosa\nimport os\n\n# Chemin de base du dataset sur Kaggle\nbase_dir = \"/kaggle/input/test-audio\"\n\n# Charger le CSV\ndf = pd.read_csv(f\"{base_dir}/Audio/metadata_balises_h1_p.csv\")\n\n# Parcourir les échantillons\nfor index, row in df.iterrows():\n    # Construire le chemin absolu du fichier audio\n    audio_path = os.path.join(base_dir, row[\"audio\"])\n    transcription = row[\"transcription\"]\n    html = row[\"html\"]\n\n    # Vérifier que le fichier audio existe avant de le charger\n    if os.path.exists(audio_path):\n        # Charger l'audio avec librosa\n        audio, sr = librosa.load(audio_path, sr=16000)  # 16kHz recommandé pour Whisper\n        print(f\"Audio: {row['audio']}, Transcription: {transcription}, HTML: {html}\")\n    else:\n        print(f\"Fichier audio manquant : {audio_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T11:31:31.717905Z","iopub.execute_input":"2025-06-04T11:31:31.718138Z","iopub.status.idle":"2025-06-04T11:31:43.761171Z","shell.execute_reply.started":"2025-06-04T11:31:31.718121Z","shell.execute_reply":"2025-06-04T11:31:43.760272Z"}},"outputs":[{"name":"stdout","text":"Audio: Audio/16k_h11.wav, Transcription: balise h1 contenu Titre Principal, HTML: <h1>Titre Principal</h1>\nAudio: Audio/16k_h12.wav, Transcription: balise h1 class=titre-Principal contenu Accueil, HTML: <h1 class=\"titre-Principal\">Accueil</h1>\nAudio: Audio/16k_h13.wav, Transcription: balise h1 id=entete contenu Bienvenue, HTML: <h1 id=\"entete\">Bienvenue</h1>\nAudio: Audio/16k_h14.wav, Transcription: balise h1 style=color:blue contenu Promotions, HTML: <h1 style=\"color:blue\">Promotions</h1>\nAudio: Audio/16k_h15.wav, Transcription: balise h1 contenu Nouveautés, HTML: <h1>Nouveautés</h1>\nAudio: Audio/16k_h16.wav, Transcription: balise h1 class=section contenu À propos, HTML: <h1 class=\"section\">À propos</h1>\nAudio: Audio/16k_h17.wav, Transcription: balise h1 id=entete-Principal contenu Services, HTML: <h1 id=\"entete\">Services</h1>\nAudio: Audio/16k_h18.wav, Transcription: balise h1 style=font-size contenu Contact, HTML: <h1 style=\"font-size\">Contact</h1>\nAudio: Audio/16k_h19.wav, Transcription: balise h1 contenu FAQ, HTML: <h1>FAQ</h1>\nAudio: Audio/16k_h110.wav, Transcription: balise h1 class=titre contenu Mentions Légales, HTML: <h1 class=\"titre\">Mentions Légales</h1>\nAudio: Audio/16k_p1.wav, Transcription: balise p contenu Bonjour comment tu vas, HTML: <p>Bonjour comment tu vas</p>\nAudio: Audio/16k_p2.wav, Transcription: balise p class=intro contenu Bienvenue, HTML: balise p class=intro contenu Bienvenue\nAudio: Audio/16k_p3.wav, Transcription: balise p id=desc contenu Description du produit, HTML: <p id=\"desc\">Description du produit</p>\nAudio: Audio/16k_p4.wav, Transcription: balise p style=color:red contenu Attention, HTML: <p style=\"color:red\">Attention</p>\nAudio: Audio/16k_p5.wav, Transcription: balise p contenu Texte standard, HTML: <p>Texte standard</p>\nAudio: Audio/16k_p6.wav, Transcription: balise p class=small contenu Note importante, HTML: <p class=\"small\">Note importante</p>\nAudio: Audio/16k_p7.wav, Transcription: balise p id=footer contenu Copyright 2023, HTML: <p id=\"footer\">Copyright 2023</p>\nAudio: Audio/16k_p8.wav, Transcription: balise p style=margin:10px contenu Réservez maintenant, HTML: <p style=\"margin:10px\">Réservez maintenant</p>\nAudio: Audio/16k_p9.wav, Transcription: balise p contenu Contactez-nous, HTML: <p>Contactez-nous</p>\nAudio: Audio/16k_p10.wav, Transcription: balise p class=alert contenu Stock limité, HTML: <p class=\"alert\">Stock limité</p>\n","output_type":"stream"}],"execution_count":2},{"id":"c5f43091","cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset, Audio\nimport os\n\ndef prepare_data(csv_path, audio_base_dir):\n    df = pd.read_csv(csv_path)\n    data = []\n    \n    for _, row in df.iterrows():\n        audio_path = os.path.join(audio_base_dir, row['audio'])\n        if not os.path.exists(audio_path):\n            print(f\"Fichier introuvable : {audio_path}\")\n            continue\n            \n        data.append({\n            \"audio\": audio_path,\n            \"text\": row[\"transcription\"]\n        })\n    \n    dataset = Dataset.from_dict({\n        \"audio\": [d[\"audio\"] for d in data],\n        \"text\": [d[\"text\"] for d in data]\n    })\n    \n    # Rééchantillonnage audio à 16 kHz\n    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n    return dataset\n\n# Définition des chemins avec base audio\ncsv_path = \"/kaggle/input/test-audio/Audio/metadata_balises_h1_p.csv\"\naudio_base_dir = \"/kaggle/input/test-audio\"\n\ndataset = prepare_data(csv_path, audio_base_dir)\n\n# Attention : dans Kaggle, tu ne peux pas écrire dans le dossier courant\n# dataset.save_to_disk(\"whisper_dataset\")  # ça risque de planter ici\n\n# Tu peux éventuellement sauvegarder sur /kaggle/working qui est accessible en écriture :\ndataset.save_to_disk(\"/kaggle/working/whisper_dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T11:31:43.762093Z","iopub.execute_input":"2025-06-04T11:31:43.762620Z","iopub.status.idle":"2025-06-04T11:31:45.228947Z","shell.execute_reply.started":"2025-06-04T11:31:43.762598Z","shell.execute_reply":"2025-06-04T11:31:45.228173Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/20 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e857915d7404e679ca4fb5528fb15c0"}},"metadata":{}}],"execution_count":3},{"id":"11761351-d148-427d-98e0-046a8525e28d","cell_type":"code","source":"!pip install -q openai-whisper","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T11:31:45.230443Z","iopub.execute_input":"2025-06-04T11:31:45.230652Z","iopub.status.idle":"2025-06-04T11:33:01.686198Z","shell.execute_reply.started":"2025-06-04T11:31:45.230636Z","shell.execute_reply":"2025-06-04T11:33:01.685394Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":4},{"id":"3de50081","cell_type":"code","source":"import whisper\nimport pandas as pd\nimport os\n\n# 1. Charger le modèle Whisper\nmodel = whisper.load_model(\"base\")\n\n# 2. Lire le CSV\ncsv_path = \"/kaggle/input/test-audio/Audio/metadata_balises_h1_p.csv\"\ndf = pd.read_csv(csv_path)\n\n# 3. Tester chaque enregistrement\nfor index, row in df.iterrows():\n    audio_file = os.path.join(\"/kaggle/input/test-audio\", row[\"audio\"])  # Correction ici\n    expected_text = row[\"transcription\"]\n    \n    if not os.path.exists(audio_file):\n        print(f\"Fichier audio manquant : {audio_file}\")\n        continue\n\n    result = model.transcribe(audio_file, language=\"fr\", task=\"transcribe\")\n    transcript = result[\"text\"].strip()\n\n    print(f\"\\nFichier : {audio_file}\")\n    print(f\"Attendu : {expected_text}\")\n    print(f\"Obtenu  : {transcript}\")\n\n    match = \"✅\" if expected_text.lower() == transcript.lower() else \"❌\"\n    print(f\"Correspondance : {match}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T11:33:01.687198Z","iopub.execute_input":"2025-06-04T11:33:01.687430Z","iopub.status.idle":"2025-06-04T11:33:20.691152Z","shell.execute_reply.started":"2025-06-04T11:33:01.687410Z","shell.execute_reply":"2025-06-04T11:33:20.690390Z"}},"outputs":[{"name":"stderr","text":"100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 153MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"\nFichier : /kaggle/input/test-audio/Audio/16k_h11.wav\nAttendu : balise h1 contenu Titre Principal\nObtenu  : BallyzH1, grand tenure t2 principale.\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_h12.wav\nAttendu : balise h1 class=titre-Principal contenu Accueil\nObtenu  : Bali, H1, classe titre principal, continue à c'est\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_h13.wav\nAttendu : balise h1 id=entete contenu Bienvenue\nObtenu  : Baleeze elements et il a souvent antennae\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_h14.wav\nAttendu : balise h1 style=color:blue contenu Promotions\nObtenu  : Valise-là un style color blue. Continue promotion.\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_h15.wav\nAttendu : balise h1 contenu Nouveautés\nObtenu  : Valise h1 nope Now울K\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_h16.wav\nAttendu : balise h1 class=section contenu À propos\nObtenu  : Bâler le H1 class section continue à propos.\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_h17.wav\nAttendu : balise h1 id=entete-Principal contenu Services\nObtenu  : Valier H1 id en tête principale, continue, c'est un vis.\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_h18.wav\nAttendu : balise h1 style=font-size contenu Contact\nObtenu  : Baleise-hache un style von size. Continue contact.\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_h19.wav\nAttendu : balise h1 contenu FAQ\nObtenu  : Valise as un contenu F-AQ\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_h110.wav\nAttendu : balise h1 class=titre contenu Mentions Légales\nObtenu  : Valise H1 classe titre continue, mention légale.\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_p1.wav\nAttendu : balise p contenu Bonjour comment tu vas\nObtenu  : Valisez-pé, continue, bonjour, comment tu vas ?\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_p2.wav\nAttendu : balise p class=intro contenu Bienvenue\nObtenu  : 觸角, classe et entreau. Continue bienvenue\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_p3.wav\nAttendu : balise p id=desc contenu Description du produit\nObtenu  : Bâler le PIDUSC continue Descriptions du produit\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_p4.wav\nAttendu : balise p style=color:red contenu Attention\nObtenu  : Valisez-pé style colorette, continue attention.\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_p5.wav\nAttendu : balise p contenu Texte standard\nObtenu  : Rel paranoid, continueground.\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_p6.wav\nAttendu : balise p class=small contenu Note importante\nObtenu  : Valise P. Class Small continue Not important.\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_p7.wav\nAttendu : balise p id=footer contenu Copyright 2023\nObtenu  : Bali est le P, ID, FOOT, continue recuperate bat à din en spricht! D conducts papers ont tué leszię, suitent les нар Mann-Rogans d'��iques et des coach psychologiques, wondering de deputy cloves,\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_p8.wav\nAttendu : balise p style=margin:10px contenu Réservez maintenant\nObtenu  : Balisez le p, stile, margin, 10 pixels, continue, réservez maintenant.\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_p9.wav\nAttendu : balise p contenu Contactez-nous\nObtenu  : Valisez le pique, continue, contactez-nous.\nCorrespondance : ❌\n\nFichier : /kaggle/input/test-audio/Audio/16k_p10.wav\nAttendu : balise p class=alert contenu Stock limité\nObtenu  : Valisope, de base à l'aide Stop, limitée\nCorrespondance : ❌\n","output_type":"stream"}],"execution_count":5},{"id":"fd885fef","cell_type":"code","source":"from datasets import load_dataset, Audio\nimport os\n\ncsv_path = \"/kaggle/input/test-audio/Audio/metadata_balises_h1_p.csv\"\naudio_base_dir = \"/kaggle/input/test-audio/Audio\"\n\n# 1. Charger le dataset\ndataset = load_dataset(\"csv\", data_files=csv_path)[\"train\"]\n\n# 2. Corriger les chemins audio pour éviter le doublon \"Audio/Audio\"\ndef fix_audio_path(example):\n    audio_path = example[\"audio\"]\n    # Si le chemin commence par \"Audio/\", on l'enlève pour éviter le doublon\n    if audio_path.startswith(\"Audio/\"):\n        audio_path = audio_path[len(\"Audio/\"):]\n    example[\"audio\"] = os.path.join(audio_base_dir, audio_path)\n    return example\n\ndataset = dataset.map(fix_audio_path)\n\n# 3. Cast colonne audio\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n\n# 4. Vérification des chemins\ndef check_audio_paths(example):\n    if isinstance(example[\"audio\"], dict):\n        path = example[\"audio\"][\"path\"]\n    else:\n        path = example[\"audio\"]\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Fichier {path} introuvable\")\n    return example\n\ndataset = dataset.map(check_audio_paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T11:33:20.692097Z","iopub.execute_input":"2025-06-04T11:33:20.692784Z","iopub.status.idle":"2025-06-04T11:33:22.976201Z","shell.execute_reply.started":"2025-06-04T11:33:20.692760Z","shell.execute_reply":"2025-06-04T11:33:22.975676Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81e314dac99c417a80bde0d645f7a4eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35eb3a1062374e50980c15707758e7e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a2f2ffde3604b269551ee9a55cbad4b"}},"metadata":{}}],"execution_count":6},{"id":"5e0f8d5a","cell_type":"code","source":"dataset.save_to_disk(\"dataset_processed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T11:33:22.976947Z","iopub.execute_input":"2025-06-04T11:33:22.977212Z","iopub.status.idle":"2025-06-04T11:33:23.006039Z","shell.execute_reply.started":"2025-06-04T11:33:22.977188Z","shell.execute_reply":"2025-06-04T11:33:23.005393Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/20 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"838c98d64af143a69a00110ac0a58b55"}},"metadata":{}}],"execution_count":7},{"id":"b8e7d7eb-0905-4e28-9762-f35758b385c3","cell_type":"code","source":"from transformers import (\n    WhisperForConditionalGeneration,\n    WhisperProcessor,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer\n)\nfrom datasets import load_from_disk\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\n\n# 1. Chargement des données\ndataset = load_from_disk(\"dataset_processed\")\n\n# 2. Initialisation du modèle\nmodel_name = \"openai/whisper-small\"\nprocessor = WhisperProcessor.from_pretrained(model_name, language=\"fr\", task=\"transcribe\")\nmodel = WhisperForConditionalGeneration.from_pretrained(model_name)\n\n# 3. Préparation des batches (suppression de tout print de débogage)\ndef prepare_batch(batch):\n    audio = batch[\"audio\"]\n    input_features = processor(\n        audio[\"array\"], \n        sampling_rate=audio[\"sampling_rate\"],\n        return_tensors=\"pt\"\n    ).input_features[0]  # Accès au tensor directement\n    \n    labels = processor.tokenizer(batch[\"transcription\"], return_tensors=\"pt\").input_ids[0]\n    \n    return {\n        \"input_features\": input_features,\n        \"labels\": labels\n    }\n\n# Appliquer la préparation des batches\ndataset = dataset.map(prepare_batch)\n\n# 4. Data Collator (conserver les impressions pour débogage si nécessaire)\ndef data_collator(features):\n    input_features = torch.stack([torch.tensor(f[\"input_features\"]) for f in features])\n    \n    # Gestion du padding des labels\n    labels = [torch.tensor(f[\"labels\"]) for f in features]\n    labels = pad_sequence(labels, batch_first=True, padding_value=processor.tokenizer.pad_token_id)\n    \n    # Débogage : vérifier les formes (peut être commenté si plus nécessaire)\n    print(\"Input features shape:\", input_features.shape)\n    print(\"Labels shape:\", labels.shape)\n    \n    return {\n        \"input_features\": input_features,\n        \"labels\": labels\n    }\n\n# 5. Configuration de l'entraînement\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./whisper_finetuned_html\",\n    run_name=\"whisper_finetune_run_1\",  # Nom unique pour WandB\n    per_device_train_batch_size=1,  # Taille réduite pour éviter les problèmes de mémoire\n    learning_rate=1e-5,\n    num_train_epochs=3,\n    fp16=True,  # Précision mixte pour optimiser la mémoire GPU\n    save_steps=100,\n    logging_steps=10,  # Log fréquent pour surveiller\n    logging_strategy=\"steps\",\n    logging_first_step=True,\n    report_to=\"none\",  # Désactiver WandB (retirer si vous utilisez WandB)\n)\n\n# 6. Vérification du dataset avant entraînement\nprint(\"Taille du dataset :\", len(dataset))\n#print(\"Exemple de données :\", dataset[0])\n\n# 7. Lancement de l'entraînement\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,\n    processing_class=processor,  # Utilisation de processing_class pour éviter FutureWarning\n    data_collator=data_collator,\n)\n\n# S'assurer que le modèle n'est pas enveloppé dans DataParallel (pour éviter l'avertissement _functions.py)\nif torch.cuda.device_count() > 1:\n    print(\"Multiple GPUs detected, but using single GPU to avoid parallel warning.\")\n    model = model.to(\"cuda:0\")\nelse:\n    model = model.to(\"cuda\")\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T11:33:23.006767Z","iopub.execute_input":"2025-06-04T11:33:23.007426Z","iopub.status.idle":"2025-06-04T11:34:29.504309Z","shell.execute_reply.started":"2025-06-04T11:33:23.007404Z","shell.execute_reply":"2025-06-04T11:34:29.503326Z"}},"outputs":[{"name":"stderr","text":"2025-06-04 11:33:30.931682: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749036811.098040      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749036811.148031      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e753cf5f58ab401283f1728318a897ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ff572c5e46945c4af9fbc51c18fe0d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beb9794e469f4a74b49dbc77fb7d1e0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"763161fe62624299ab487570de1d5164"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d827c4735a5d4312af381765be7153f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c7c90ab1a444f128dfba03ca287ce03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8760c5e3c8934c04b3793642f66cf6c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30b937172ce442869082827635eaba05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52eeba17046749ce97005847cccc5ca3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ce3dddf7320463295356900e2e07e8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/3.87k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb7a3d67c1194f47b35e8fdc65003d1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7878921b345340f7a0fa7d22da213e4f"}},"metadata":{}},{"name":"stdout","text":"Taille du dataset : 20\nMultiple GPUs detected, but using single GPU to avoid parallel warning.\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 17])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 16])\n","output_type":"stream"},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30/30 00:33, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>5.719300</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.549900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.887600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.618000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Input features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 23])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 15])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 21])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 19])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 23])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 18])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 17])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 20])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 17])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 17])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 20])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 23])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 15])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 18])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 21])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 17])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 23])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 17])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 18])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 20])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 16])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 21])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 17])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 19])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 23])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 17])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 16])\nInput features shape: torch.Size([2, 80, 3000])\nLabels shape: torch.Size([2, 23])\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=30, training_loss=1.4574635028839111, metrics={'train_runtime': 35.8534, 'train_samples_per_second': 1.673, 'train_steps_per_second': 0.837, 'total_flos': 1.73151240192e+16, 'train_loss': 1.4574635028839111, 'epoch': 3.0})"},"metadata":{}}],"execution_count":8},{"id":"d621bdd2-0e71-43ee-90c7-ea7bbcd41370","cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T11:34:29.505396Z","iopub.execute_input":"2025-06-04T11:34:29.505729Z","iopub.status.idle":"2025-06-04T11:34:33.615225Z","shell.execute_reply.started":"2025-06-04T11:34:29.505701Z","shell.execute_reply":"2025-06-04T11:34:33.614279Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2025.3.0\n","output_type":"stream"}],"execution_count":9},{"id":"afb8b2ee-5502-4d0f-b7b5-bff87301c199","cell_type":"code","source":"from transformers import pipeline, WhisperForConditionalGeneration, WhisperProcessor\nfrom datasets import load_from_disk\nimport evaluate\nimport torch\n\n# 1. Charger le processeur et le modèle fine-tuné\ncheckpoint_dir = \"./whisper_finetuned_html/checkpoint-30\"  # Remplacez par le bon checkpoint si différent\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"fr\", task=\"transcribe\")\nmodel = WhisperForConditionalGeneration.from_pretrained(checkpoint_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T11:34:33.618035Z","iopub.execute_input":"2025-06-04T11:34:33.618551Z","iopub.status.idle":"2025-06-04T11:34:34.782102Z","shell.execute_reply.started":"2025-06-04T11:34:33.618521Z","shell.execute_reply":"2025-06-04T11:34:34.781303Z"}},"outputs":[],"execution_count":10},{"id":"1afb7ec4-a1e0-4294-9c43-b023c7d472d9","cell_type":"code","source":"# 2. Configurer la pipeline\ntranscriber = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T11:34:34.782815Z","iopub.execute_input":"2025-06-04T11:34:34.783020Z","iopub.status.idle":"2025-06-04T11:34:35.126143Z","shell.execute_reply.started":"2025-06-04T11:34:34.783004Z","shell.execute_reply":"2025-06-04T11:34:35.125375Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda\n","output_type":"stream"}],"execution_count":11},{"id":"fa177eee-279a-436f-a774-8bb2ba5dbe18","cell_type":"code","source":"# 3. Charger le dataset (votre dataset local)\ndataset = load_from_disk(\"dataset_processed\")  # Remplacez par le chemin de votre dataset de test si différent","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T11:35:21.849273Z","iopub.execute_input":"2025-06-04T11:35:21.850052Z","iopub.status.idle":"2025-06-04T11:35:21.856268Z","shell.execute_reply.started":"2025-06-04T11:35:21.850028Z","shell.execute_reply":"2025-06-04T11:35:21.855658Z"}},"outputs":[],"execution_count":14},{"id":"6d4c705b-95d2-4cef-8666-8678a5698576","cell_type":"code","source":"!pip install jiwer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T11:43:16.237719Z","iopub.execute_input":"2025-06-04T11:43:16.238519Z","iopub.status.idle":"2025-06-04T11:43:20.999090Z","shell.execute_reply.started":"2025-06-04T11:43:16.238492Z","shell.execute_reply":"2025-06-04T11:43:20.998367Z"}},"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\nCollecting rapidfuzz>=3.9.7 (from jiwer)\n  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nDownloading jiwer-3.1.0-py3-none-any.whl (22 kB)\nDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\nSuccessfully installed jiwer-3.1.0 rapidfuzz-3.13.0\n","output_type":"stream"}],"execution_count":19},{"id":"3bef9889-f7be-4659-93fa-c0e0338ccee7","cell_type":"code","source":"from transformers import pipeline, WhisperForConditionalGeneration, WhisperProcessor, GenerationConfig\nfrom datasets import load_from_disk\nimport evaluate\nimport torch\nimport numpy as np\n\n# 1. Charger le processeur et le modèle fine-tuné\ncheckpoint_dir = \"./whisper_finetuned_html/checkpoint-30\"  # Remplacez par le bon checkpoint si différent\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"fr\", task=\"transcribe\")\nmodel = WhisperForConditionalGeneration.from_pretrained(checkpoint_dir)\n\n# 2. Réinitialiser la configuration de génération pour éviter les conflits\nmodel.generation_config = GenerationConfig.from_pretrained(\n    \"openai/whisper-small\",\n    language=\"french\",\n    task=\"transcribe\",\n    forced_decoder_ids=None  # Désactiver explicitement forced_decoder_ids\n)\n\n# 3. Configurer la pipeline\ntranscriber = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\n\n# 4. Charger le dataset\ndataset = load_from_disk(\"dataset_processed\")  # Remplacez par le chemin de votre dataset de test si différent\n\n# 5. Test sur un exemple\naudio_sample = dataset[\"audio\"][0]\ntranscription = transcriber(\n    audio_sample[\"array\"],  # Passer directement le tableau NumPy brut\n    generate_kwargs={\n        \"language\": \"french\",\n        \"task\": \"transcribe\",\n        \"forced_decoder_ids\": None  # Désactiver forced_decoder_ids\n    }\n)\nprint(f\"Résultat de la transcription : {transcription['text']}\")\nprint(f\"Transcription réelle : {dataset['transcription'][0]}\")\n\n# 6. Évaluer avec la métrique WER (Word Error Rate)\nwer_metric = evaluate.load(\"wer\")\npredictions = []\nreferences = dataset[\"transcription\"]\n\nfor audio in dataset[\"audio\"]:\n    pred = transcriber(\n        audio[\"array\"],  # Passer directement le tableau NumPy brut\n        generate_kwargs={\n            \"language\": \"french\",\n            \"task\": \"transcribe\",\n            \"forced_decoder_ids\": None  # Désactiver forced_decoder_ids\n        }\n    )[\"text\"]\n    predictions.append(pred)\n\nwer = wer_metric.compute(predictions=predictions, references=references)\nprint(f\"WER : {wer:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T11:43:23.841464Z","iopub.execute_input":"2025-06-04T11:43:23.841767Z","iopub.status.idle":"2025-06-04T11:43:32.181363Z","shell.execute_reply.started":"2025-06-04T11:43:23.841743Z","shell.execute_reply":"2025-06-04T11:43:32.180610Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda\n","output_type":"stream"},{"name":"stdout","text":"Résultat de la transcription : balise h1 contenu Titre Principal\nTranscription réelle : balise h1 contenu Titre Principal\n","output_type":"stream"},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"name":"stdout","text":"WER : 22.43%\n","output_type":"stream"}],"execution_count":20},{"id":"22864cba-ec49-47ca-9aac-434d8d8f13ee","cell_type":"code","source":"pmspm s pms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T11:34:36.606855Z","iopub.status.idle":"2025-06-04T11:34:36.607196Z","shell.execute_reply.started":"2025-06-04T11:34:36.607023Z","shell.execute_reply":"2025-06-04T11:34:36.607037Z"}},"outputs":[],"execution_count":null}]}